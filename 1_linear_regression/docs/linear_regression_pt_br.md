Antes de entendermos o que √© uma regress√£o linear, precisamos entender:

## Bias

Bias - √â um n√∫mero que √© adicionado depois da multiplica√ß√£o dos valores de entrada e pesos, ele 
serve como ponto de partida para que a fun√ß√£o possa ser aplicada de maneira √≥tima.

Exemplo: Se a fun√ß√£o tenta demonstrar o valor de um carro ao decorrer dos anos, o bias teria que
ser positivo, j√° que o valor do carro n√£o pode ser negativo em nenhum momento.

o Bias existe para servir como um ponto de in√≠cio.

![img.png](resources/img.png)

## Peso

Peso - √â o multiplicador que o n√∫mero de entrada recebe, √© o que determina a influ√™ncia de cada 
valor no resultado final.

## Regress√£o linear

A regress√£o linear √© uma t√©cnica que visa tra√ßar uma linha ideal que possa ser o mais pr√≥ximo poss√≠vel 
de todos os pontos de dados que est√£o sendo analisados.

Matem√°ticamente falando, a regress√£o linear √© descrita pela seguinte fun√ß√£o alg√©brica:

$$
\hat{y} = b + x \cdot w
$$

onde:

* esse Y com chap√©u estranho √© o resultado em que queremos chegar
* b - √© o bias, ou vi√©s
* x - √© o valor de entrada
* w - √© o peso atribu√≠do ao valor de entrada

# Fun√ß√µes de perda.

A perda no contexto de estat√≠stica e aprendizado de m√°quina, √© uma m√©trica usada para avaliar a dist√¢ncia entre o resultado
previsto, e o resultado final.

Um adendo importante √© que a perda se importa com a diferen√ßa entre os valores, e n√£o na dire√ß√£o, ou seja
**TODOS** os resultados de uma fun√ß√£o de perda ser√£o n√£o negativos (positivos + 0), j√° que na matem√°tica, o sinal pode representar a dire√ß√£o em um eixo.

Durante o treinamento, os valores de B e W s√£o ajustados para diminuir a dist√¢ncia entre os valores 
previstos e valores reais.

## Tipos de fun√ß√µes de perda


### Loss (L1): 
Essa √© a soma dos valores absolutos (ou seja, s√≥ valores positivos) da diferen√ßa entre os 
valores previstos e valores reais.

$$ 
\sum_{i=1}^{n} |valor real - valor previsto|
$$

ou para os programadores de plant√£o:

```python

valores_reais = [1,2,3]
valores_previstos = [4,5,6]

loss = 0

for real_value, predicted_value in zip(valores_reais, valores_previstos):
    loss += abs(real_value - predicted_value)

print(loss)
```

Em geral, o L1 tende a **zerar** pesos de vari√°veis menos importantes para o c√°lculo,
supondo que o conjunto de dados tem muitas features (ou seja, colunas ou campos do dataset), 
o L1 tamb√©m conhecido como lasso vai eliminar o peso das vari√°veis que menos afetam o resultado final.

Idealmente, o L1 deve ser usado quando temos muitas features de alta e baixa qualidade misturadas.
Entretanto, o Lasso tende a performar mal em casos de overfitting comparado com o L2


---

### Squared Loss (L2)

O L2 √© bem parecido com o L1, com a diferen√ßa que o loss √© elevado ao quadrado.

$$
\sum_{i=1}^{n} (valor real - valor previsto)^2
$$

O L2 ou Ridge tende a dividir bem os pesos entre as vari√°veis, seu uso √© recomendado
em casos de risco de overfitting

---

### Erro m√©dio absoluto (MAE):

O erro m√©dio absoluto n√£o √© nada mais nada menos que a m√©dia de loss.

$$
\frac{1}{n} \sum_{i=1}^{n} |valor real - valor previsto|
$$

ou em python:

```python


valores_reais = [1,2,3]
valores_previstos = [4,5,6]

loss = 0

for real_value, predicted_value in zip(valores_reais, valores_previstos):
    loss += abs(real_value - predicted_value)

# O loss √© dividido pela quantidade de dados
loss = loss / len(valores_reais)

```

O MAE √© usado para penalizar todos os erros igualmente, ele performa muito bem
em conjuntos com muitos outliers, ou seja, conjuntos que existem muitos pontos fora da curva

O MSE move o modelo mais para os valores discrepantes

### Erro quadr√°tico medio (MSE):

Assim como L1 est√° para L2, MAE est√° para MSE.

$$
\frac{1}{n} \sum_{i=1}^{n} (valor real - valor previsto)^2
$$

Ent√£o basta elevar ao quadrado antes de fazer a m√©dia.

O MSE √© usado para penalizar fortemente os erros como um todo,
ele √© usado para tentar alcan√ßar eficiencia m√°xima no conjunto,
tendo um alto risco de overfitting e de ser influenciado por outliers,
j√° que o MSE move a linha para perto dos valores discrepantes.

---

Em geral, m√©todos que s√£o elevados a alguma potencia tendem a ser muito
afetados por outliers, j√° que os outliers s√£o extremamente punidos, assim, 
gerando uma curva que tende a se aproximar muito desses n√∫meros fora da curva.


## Refer√™ncia

- [üé• Finding Derivative of L1, L2, ReLU... (YouTube)](https://www.youtube.com/watch?v=vF2Q69V0ThA)